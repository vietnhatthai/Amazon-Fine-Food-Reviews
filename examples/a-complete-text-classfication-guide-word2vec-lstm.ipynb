{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "27bfe1228e31840aee73ac741fa498c8fe227a82"
   },
   "source": [
    "## Text Classification on Amazon Fine Food Dataset with Google Word2Vec Word Embeddings in Gensim and training using LSTM In Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89acce557b54ba7cb0ebf85fb3bf22d9e95868a6"
   },
   "source": [
    "### IMPORTING THE MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "739312197e54166212116baab4af1d81fb4e9ccb",
    "id": "u4Qoy1EB5hmJ",
    "outputId": "9fc57d33-b6cc-45c1-c4ec-24394bb61544"
   },
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "# % matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# BeautifulSoup libraray\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import re # regex\n",
    "\n",
    "#model_selection\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score \n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "#preprocessing scikit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "#classifiaction.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    " \n",
    "#stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#keras\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#gensim w2v\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cff40d5497523fa8a1815323f6684c66fc1d008e"
   },
   "source": [
    "### LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "1abe90648e9f876e84a5ca74717e373d9877c9ab",
    "id": "PxRv4O4w6ELq"
   },
   "outputs": [],
   "source": [
    "rev_frame=pd.read_csv(r'../data/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "91085adcafd8bc3ecfbd898fe7d3536aea36da49",
    "id": "RmCtzv5O6UWo"
   },
   "outputs": [],
   "source": [
    "df=rev_frame.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7814bbb9e96368171a34663280d4dac827d27a37",
    "id": "QiKI5fM664D9",
    "outputId": "2129c76c-53cb-4c43-9a47-15896f90052f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0a9b516821934fff9510beb08e38f854c1204a9"
   },
   "source": [
    "#### A brief description of the dataset from Overview tab on Kaggle : -\n",
    "\n",
    "Data includes:\n",
    "- Reviews from Oct 1999 - Oct 2012\n",
    "- 568,454 reviews\n",
    "- 256,059 users\n",
    "- 74,258 products\n",
    "- 260 users with > 50 reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a92034bd40cb8f1f2e3adf17711001d1d8978da",
    "id": "3Cqvk7IpDsqI"
   },
   "source": [
    "### DATA CLEANING AND PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8942ed66c74519fe2d4dcb54d144f66e2b45b675"
   },
   "source": [
    "#### Since here I am concerned with sentiment analysis I shall keep only the 'Text' and the 'Score' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "e6d5d863d0efb7bc29ba4f1a2fb51f1e3917e248",
    "id": "04Lhx1IhD4MK"
   },
   "outputs": [],
   "source": [
    "df=df[['Text','Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "9a80c3ae0e318bb9a95071f8aef3205aac25d1e3",
    "id": "fD_pd159D8Yt"
   },
   "outputs": [],
   "source": [
    "df['review']=df['Text']\n",
    "df['rating']=df['Score']\n",
    "df.drop(['Text','Score'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "a7e69cc4da9b9b42a1b09556526ad82b0d16a506",
    "id": "ev-hm43hEZkA",
    "outputId": "324a13fa-1aaf-4ea2-f461-e04a078cfe13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568454, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating\n",
       "0  I have bought several of the Vitality canned d...       5\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...       1\n",
       "2  This is a confection that has been around a fe...       4\n",
       "3  If you are looking for the secret ingredient i...       2\n",
       "4  Great taffy at a great price.  There was a wid...       5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66f8562443f098c8507486bf3886d3a19dc582e6",
    "id": "hFLpZZSWEr_q"
   },
   "source": [
    "#### Let us now see if any of the column has any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "218cb4ec27d0b4a1509d436f1c26bdb6221e15b7",
    "id": "b9UiXK5PFTYQ",
    "outputId": "d1700272-b7e5-416c-e5c5-6baee1d2928d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "print(df['rating'].isnull().sum())\n",
    "df['review'].isnull().sum()  # no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7d8261eda8aaf1e28a5952ad7a71e1d906ecc12"
   },
   "source": [
    "#### Note that there is no point for keeping rows with different scores or sentiment for same review text.  So I will keep only one instance and drop the rest of the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "ab0a87b8287034de77b2c699d19ab1737b9a15b9",
    "id": "xcccEV8TRfrc"
   },
   "outputs": [],
   "source": [
    "# remove duplicates/ for every duplicate we will keep only one row of that type. \n",
    "df.drop_duplicates(subset=['rating','review'],keep='first',inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "b3b2228a4690d40c3ee93695f23cbf3656280b31",
    "id": "IieegGjeRsUx",
    "outputId": "31a4554b-fa6c-4869-b52a-a12b9228a588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(393675, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating\n",
       "0  I have bought several of the Vitality canned d...       5\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...       1\n",
       "2  This is a confection that has been around a fe...       4\n",
       "3  If you are looking for the secret ingredient i...       2\n",
       "4  Great taffy at a great price.  There was a wid...       5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now check the shape. note that shape is reduced which shows that we did has duplicate rows.\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd6a139e98f2a280753454f5ba81700391d5dc33"
   },
   "source": [
    "#### Let us now print some reviews and see if we can get insights from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "7a777750ba4765f0bd3087766edbaedf51713490",
    "id": "uiFFGnZ28lSs",
    "outputId": "92ac6d5e-6756-4303-944a-b2c24e5330cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "\n",
      "\n",
      "Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "\n",
      "\n",
      "This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "\n",
      "\n",
      "If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
      "\n",
      "\n",
      "Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing some reviews to see insights.\n",
    "for review in df['review'][:5]:\n",
    "    print(review+'\\n'+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97cd13c873b9c70bb5a0a1b1850563cbabfa78f5"
   },
   "source": [
    "#### There is nothing much that I can figure out except the fact that there are some stray words and some punctuation that we have to remove before moving ahead.\n",
    "\n",
    "**But note that if I remove the punctuation now then it will be difficult to break the reviews into sentences which is required by Word2Vec constructor in Gensim. So we will first break text into sentences and then clean those sentences. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a718df2ddf66e154aab7cce2607b3af4965c79ec"
   },
   "source": [
    "#### Note that since we are doing sentiment analysis I will convert the values in score column to sentiment. Sentiment is 0 for ratings or scores less than 3 and 1 or  +  elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f34d0ca5176df79971e63d2b283ee890c7743618",
    "id": "cK5HuI9H_MFf"
   },
   "outputs": [],
   "source": [
    "def mark_sentiment(rating):\n",
    "  if(rating<=3):\n",
    "    return 0\n",
    "  else:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "53400cfb2c759e19aa920dfe8c993d7511d46fcd",
    "id": "yHsAqyByBJsn"
   },
   "outputs": [],
   "source": [
    "df['sentiment']=df['rating'].apply(mark_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "bd14ee54153c88deaf2d3ad58e3e8178c17662e2",
    "id": "xkp3SW43Bh1a"
   },
   "outputs": [],
   "source": [
    "df.drop(['rating'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "c23230c64d8f3b6c58cead9b0ef295cf6ed0fe7f",
    "id": "OUYcxtKOBnpL",
    "outputId": "d9a332aa-ff38-4c1f-b9ff-04ad0fcee6a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I have bought several of the Vitality canned d...          1\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...          0\n",
       "2  This is a confection that has been around a fe...          1\n",
       "3  If you are looking for the secret ingredient i...          0\n",
       "4  Great taffy at a great price.  There was a wid...          1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "07a40af0e7bee10926af888aa5f21909ff72985d",
    "id": "eF78FZNuB3Lq",
    "outputId": "969fb056-2f1a-46ec-9bcd-e083e99fb373"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    306819\n",
       "0     86856\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6c6fe582fcbdb4b6a4cfa2a10c6bc5dcb589210"
   },
   "source": [
    "As you can see the sentiment column now has sentiment of the corressponding product review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4fd194b0f6814612de5a4fce805eb7649450d75",
    "id": "xlsuYHoe8wY_"
   },
   "source": [
    "#### Pre-processing steps :\n",
    "\n",
    "1 ) First **removing punctuation and html tags** if any. note that the html tas may be present ast the data must be scraped from net.\n",
    "\n",
    "2) **Tokenize** the reviews into tokens or words .\n",
    "\n",
    "3) Next **remove the stop words and shorter words** as they cause noise.\n",
    "\n",
    "4) **Stem or lemmatize** the words depending on what does better. Herer I have yse lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "d8d6aa792cfa36dcc1b33cf90213001891e7429d",
    "id": "KIm7Erd586HC"
   },
   "outputs": [],
   "source": [
    "# function to clean and pre-process the text.\n",
    "def clean_reviews(review):  \n",
    "    \n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(review,\"lxml\").get_text()\n",
    "    \n",
    "    # 2. Retaining only alphabets.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    \n",
    "    # 3. Converting to lower case and splitting\n",
    "    word_tokens= review_text.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    le=WordNetLemmatizer()\n",
    "    stop_words= set(stopwords.words(\"english\"))     \n",
    "    word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    cleaned_review=\" \".join(word_tokens)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "effe8f6f6870d20e292d2c0b91bc2629f213203b",
    "id": "tzkaC80b96I7"
   },
   "source": [
    "#### Note that pre processing all the reviews is taking way too much time and so I will take only 100K reviews. To balance the class  I have taken equal instances of each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "6b9d5cde9cac9a93ee404a67bf2e3715ea1a05d4",
    "id": "EtbatAehCwqU"
   },
   "outputs": [],
   "source": [
    "pos_df=df.loc[df.sentiment==1,:][:50000]\n",
    "neg_df=df.loc[df.sentiment==0,:][:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "b0e23f9b74e34c04e0f43f9fb2fbb34036820eed",
    "id": "Um1-wLvfEKdN",
    "outputId": "50b5fe81-1e89-415e-9f9d-e7f6d6f33f3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I have bought several of the Vitality canned d...          1\n",
       "2  This is a confection that has been around a fe...          1\n",
       "4  Great taffy at a great price.  There was a wid...          1\n",
       "5  I got a wild hair for taffy and ordered this f...          1\n",
       "6  This saltwater taffy had great flavors and was...          1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "4e194a611c91fa997d1cd62ccc77d2f7a7ce41bc",
    "id": "w_-UD-EfDMST",
    "outputId": "26db4b93-5b41-4c7c-8ded-5715b5f92e75"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My cats have been happily eating Felidae Plati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I love eating them and they are good for watch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The candy is just red , No flavor . Just  plan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  sentiment\n",
       "1   Product arrived labeled as Jumbo Salted Peanut...          0\n",
       "3   If you are looking for the secret ingredient i...          0\n",
       "12  My cats have been happily eating Felidae Plati...          0\n",
       "16  I love eating them and they are good for watch...          0\n",
       "26  The candy is just red , No flavor . Just  plan...          0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "947fd99a414149bae561fc474e3899d795282744"
   },
   "source": [
    "#### We can now combine reviews of each sentiment and shuffle them so that their order doesn't make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "7c893aaa28c80717a51e7f3d7a36479cf6d63f06",
    "id": "LmLJwhFPEEDy"
   },
   "outputs": [],
   "source": [
    "#combining\n",
    "df=pd.concat([pos_df,neg_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "f98ce122399a51e2c5279b4cde328139205c658f",
    "id": "hWFZKf3dERLU",
    "outputId": "3f22763a-2dff-47e1-eb86-5e2111d5d447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I have bought several of the Vitality canned d...          1\n",
       "1  This is a confection that has been around a fe...          1\n",
       "2  Great taffy at a great price.  There was a wid...          1\n",
       "3  I got a wild hair for taffy and ordered this f...          1\n",
       "4  This saltwater taffy had great flavors and was...          1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "46e946e4e593482f373a7ac97cda97b1310cc72a",
    "id": "ngCjsqeLHknP",
    "outputId": "2e53f95a-4cb4-413d-b6f5-5a225e3e9309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitter, bland and essentially bad.  That's wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It does not state that these bars need to be r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am an expat living in Thailand so I know a t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ordered this on a whim thru subscribe &amp; save. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These are great snacks. But realize what a sma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Bitter, bland and essentially bad.  That's wha...          0\n",
       "1  It does not state that these bars need to be r...          0\n",
       "2  I am an expat living in Thailand so I know a t...          0\n",
       "3  Ordered this on a whim thru subscribe & save. ...          0\n",
       "4  These are great snacks. But realize what a sma...          1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffling rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(df.shape)  # perfectly fine.\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2eb88a2676d7cc8c49bdcbf1b48c2c649bde10d2",
    "id": "TCFFcmQGvauc"
   },
   "source": [
    "### CREATING GOOGLE WORD2VEC WORD EMBEDDINGS IN GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a5f6b6093e1a0179a21424efcc522269551b650"
   },
   "source": [
    "In this section I have actually created the word embeddings in Gensim. Note that I planed touse the pre-trained word embeddings like the google word2vec trained on google news corpusor the famous Stanford Glove embeddings. But as soon as I load the corressponding embeddings through Gensim the runtime dies and kernel crashes ; perhaps because it contains 30L words and which is exceeding the RAM on Google Colab.\n",
    "\n",
    "Because of this ; for now I have created the embeddings by training on my own corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "a2c0d39ac71fca195acb0d3ea5d99b57c625e350",
    "id": "b8LYvfQn_GD0",
    "outputId": "2d2b504e-6f63-4f5c-8eab-8dc9b26098a7"
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# # load Google's pre-trained Word2Vec model.\n",
    "# pre_w2v_model = gensim.models.KeyedVectors.load_word2vec_format(r'drive/Colab Notebooks/amazon food reviews/GoogleNews-vectors-negative300.bin', binary=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d002231f453318a3f418a24ec2b532d063ad5bc4",
    "id": "i4RbhljSnAVq"
   },
   "source": [
    "#### First we need to break our data into sentences which is requires by the constructor of the Word2Vec class in Gensim. For this I have used Punk English tokenizer from the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "92fccc63bd1a69dba8e5729a929402c44efd2b76",
    "id": "eDa2gp9zorg2",
    "outputId": "85c59721-8e9a-40f6-df9a-f80bf3f823bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511928\n",
      "511928\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences=[]\n",
    "sum=0\n",
    "for review in df['review']:\n",
    "  sents=tokenizer.tokenize(review.strip())\n",
    "  sum+=len(sents)\n",
    "  for sent in sents:\n",
    "    cleaned_sent=clean_reviews(sent)\n",
    "    sentences.append(cleaned_sent.split()) # can use word_tokenize also.\n",
    "print(sum)\n",
    "print(len(sentences))  # total no of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e42c58b98aeac961923735a17cf0cd5e9400130"
   },
   "source": [
    "#### Now let us print some sentences just to check iff they are in the correct fornat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "9818e2ccefffde2fb5f409a13b87e44b7c14b5ca",
    "id": "HzJzXDeGqTTR",
    "outputId": "517cb158-a089-4721-fcd0-086eb11ec778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bitter', 'bland', 'essentially', 'bad'] \n",
      "\n",
      "['coffee', 'taste', 'like', 'smell', 'nice', 'open', 'chocolaty', 'mix', 'coffee', 'cocoa', 'fill', 'room', 'pleasant', 'smell', 'brewing'] \n",
      "\n",
      "['almost', 'smell', 'like', 'chocolate', 'shop', 'someone', 'running', 'nice', 'pot', 'coffee', 'back', 'giving', 'whole', 'place', 'aroma', 'winter', 'morning', 'geneva', 'something'] \n",
      "\n",
      "['gevalia', 'supposed', 'sound', 'swiss', 'right', 'bad', 'swedish'] \n",
      "\n",
      "['swiss'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trying to print few sentences\n",
    "for te in sentences[:5]:\n",
    "  print(te,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77edb6b38a7da2587bd57cfaf3dd28d8c7dcbf2e"
   },
   "source": [
    "####  Now actually creating the word 2 vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "1a3b204a1219c074721fff30b1d3912f633eab74",
    "id": "bbUhUFHCsTx_"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model=gensim.models.Word2Vec(sentences=sentences,vector_size=300,window=10,min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f2a08433d75209a955eac4dc9fb7d974a6072716"
   },
   "source": [
    "#### Parameters: -\n",
    "\n",
    "**sentences : ** The sentences we have obtained.\n",
    "\n",
    "**size : ** The dimesnions of the vector used to represent each word.\n",
    "\n",
    "**window : ** The number f words around any word to see the context.\n",
    "\n",
    "**min_count : ** The minimum number of times a word should appear for its embedding to be formed or learnt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "5f04a542e992d3e5bbd4fffd5bd6090b353a4036",
    "id": "YdpRIli7tfzu",
    "outputId": "d52b2afa-2dab-4403-db68-6d3dfc77f5ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38406632, 41193730)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sentences,epochs=10,total_examples=len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11b1f4e6ac115a89ac0307b74829ebe6895ed68d",
    "id": "Z9ZN6J_Yt9rJ"
   },
   "source": [
    "#### Now can try some things with word2vec embeddings. Thanks to Gensim ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "8ad6baab82cbba74094017af9e30e138fa278fe7",
    "id": "9FU9z2LTuBDP",
    "outputId": "0556829c-6110-4348-b233-d2589da93f0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08026601, -0.49759638,  0.914199  ,  1.2098002 ,  0.44845966,\n",
       "       -0.6744179 ,  0.32651594,  0.15628016, -0.9429385 , -1.0599388 ,\n",
       "        1.9753294 ,  1.7052262 , -1.111981  , -0.46702966, -0.50497174,\n",
       "        1.0473917 , -0.296043  , -0.12577233, -1.1880034 , -0.86477816,\n",
       "        0.6499253 ,  0.62717104,  0.4945082 ,  0.00769644, -0.13871932,\n",
       "       -0.9124517 ,  0.541383  ,  2.2895362 , -0.04440752, -0.29926202,\n",
       "       -1.1543332 ,  0.38158152,  0.22513339, -0.15807651, -0.23996499,\n",
       "        0.59936816,  0.12916295, -0.49767452, -0.7911013 , -0.5129738 ,\n",
       "       -0.6430387 , -0.08508968, -1.0966103 , -0.24482544,  0.34531137,\n",
       "        0.73058414,  1.1860492 ,  0.7936194 , -0.51768595, -0.68300617,\n",
       "       -1.94526   ,  0.67893344, -1.3160049 ,  0.31944862,  0.91573924,\n",
       "        0.04759908, -0.655804  , -0.56745684, -0.0972648 ,  2.2374697 ,\n",
       "       -0.04237108, -0.03875537, -0.4759753 ,  0.43001446,  0.7670893 ,\n",
       "       -0.9929475 , -0.30658   ,  0.39590722,  0.27077731,  1.5887898 ,\n",
       "        0.5021144 ,  0.27684373, -2.2829144 , -0.8849662 ,  1.2594277 ,\n",
       "       -0.4596957 , -0.25592417, -0.01250477, -0.35946015, -1.3203795 ,\n",
       "       -0.5613466 ,  0.0347207 ,  0.4642311 , -0.06882924,  0.81411475,\n",
       "        1.3681235 ,  1.1138034 , -0.9635336 , -0.70598274, -0.3076571 ,\n",
       "        0.92862487,  0.09439251,  0.21982549, -0.44060963,  0.7201482 ,\n",
       "       -0.71069294, -0.19378525, -0.704573  , -0.2692268 , -0.9502256 ,\n",
       "       -0.45829162,  0.14744386, -0.67927605,  0.9247851 ,  0.5060257 ,\n",
       "       -1.0268667 , -0.5828133 , -0.84317213, -0.23267113, -0.3433584 ,\n",
       "        0.09974663, -1.0502006 ,  0.27126214,  1.4672855 , -1.2960284 ,\n",
       "        0.21452846,  0.55059695,  0.32866365, -0.3783342 ,  0.41728795,\n",
       "       -0.16620754, -1.5583638 ,  0.3272977 , -0.3506515 , -0.21041316,\n",
       "       -0.71086895, -0.42225808, -0.09928955, -0.8265028 , -0.34377807,\n",
       "        0.14722642, -0.74219483, -0.10680493,  1.5003512 , -0.49646577,\n",
       "        0.20886055,  1.0312848 ,  0.5498071 ,  2.2640138 ,  1.0873677 ,\n",
       "        1.4263306 , -1.0116798 , -1.6638421 ,  0.8924394 ,  0.3822907 ,\n",
       "        0.07787631,  1.1657431 , -0.32594213,  0.2103701 ,  1.0732393 ,\n",
       "        0.8164184 , -0.33854088,  0.6865039 , -0.34246325, -0.6971377 ,\n",
       "        0.09723029,  1.0217266 ,  0.8966789 ,  0.04107469,  0.6923308 ,\n",
       "       -0.07754105,  0.16014469,  0.21296883,  0.21571821,  1.0230238 ,\n",
       "        0.5271348 ,  1.0009301 ,  0.10430047,  0.16885886, -0.3977863 ,\n",
       "        1.1757898 , -1.2070361 ,  1.0366668 ,  1.3059819 , -0.9248411 ,\n",
       "       -0.38804054,  0.22389042, -0.24280451,  0.27802664, -0.9056197 ,\n",
       "       -0.678486  ,  0.1988862 ,  1.649692  , -0.85707945, -0.49785465,\n",
       "        0.46044576, -0.98747003,  0.07848965, -0.17216995,  0.9740333 ,\n",
       "       -0.43533376,  0.09622173,  1.0811293 , -0.88061816, -1.1240119 ,\n",
       "        1.0660197 , -0.40289164, -0.19872193, -0.9612188 , -0.30097663,\n",
       "        0.7378158 , -0.14020549, -1.335265  , -0.23146024,  0.48316115,\n",
       "        0.57360774, -0.8993986 , -0.7529602 , -0.34430677,  0.43294594,\n",
       "        0.15904577,  0.22194478,  0.23310933, -0.07489798,  0.6395825 ,\n",
       "        0.4391276 ,  0.5779234 ,  0.9572122 , -1.0182624 , -0.05759805,\n",
       "       -1.451414  , -0.04743181,  0.8089382 ,  0.4663065 ,  1.1656927 ,\n",
       "       -1.1209867 ,  0.23023011,  0.60171145, -0.1925084 , -0.47289398,\n",
       "       -1.2965151 , -0.74090654,  1.3556231 , -1.531108  ,  0.20006406,\n",
       "        0.4158392 ,  0.0897487 , -0.5483819 , -0.45883474, -0.27695605,\n",
       "        0.14571068,  0.7720109 , -0.6493682 , -0.37871823,  0.8923164 ,\n",
       "       -0.10875372,  0.7423875 , -0.66900504, -0.26927245, -0.12709597,\n",
       "       -0.4212356 , -0.3106514 , -1.512123  ,  0.5388268 ,  0.8855874 ,\n",
       "       -0.6445642 , -0.6982107 , -0.5918824 , -0.65552855, -1.1217228 ,\n",
       "        0.7458484 ,  0.43743286, -0.48755738,  0.11212955,  0.5898393 ,\n",
       "        0.6705666 ,  0.5274045 ,  0.90940046, -0.13630883,  0.9328308 ,\n",
       "       -0.8701314 ,  0.10007165,  0.00596819, -0.42950764, -0.75706786,\n",
       "        0.8682576 , -0.6124347 , -1.3961096 ,  1.3337079 ,  0.78306705,\n",
       "        0.9100751 , -0.21280022, -1.0933627 ,  0.870267  , -0.02840705,\n",
       "       -0.15883556, -0.31196415, -0.85807383, -1.040063  ,  0.06166426,\n",
       "        0.11052904, -0.19027714,  0.17190981, -0.48864275,  0.25524962,\n",
       "        1.2335292 ,  1.1247797 ,  0.04677431,  1.0179824 ,  0.47428748],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding of a particular word.\n",
    "w2v_model.wv.get_vector('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "0050556ac2df4bba102776191ac0b348a1ef9bde",
    "id": "LuiNWZrHuOx8",
    "outputId": "81f04fde-f66d-4ad9-df37-ff30855ec44d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words are :  56379\n"
     ]
    }
   ],
   "source": [
    "# total numberof extracted words.\n",
    "vocab=w2v_model.wv.key_to_index\n",
    "print(\"The total number of words are : \",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "51ddd393a5cf45056dc8321c85f1ba055d0c1db4",
    "id": "VIfgI10IuakQ",
    "outputId": "67471d44-d5f7-432f-88c5-40b9ad1483d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reminded', 0.5002346634864807),\n",
       " ('strange', 0.49235332012176514),\n",
       " ('alright', 0.4790930449962616),\n",
       " ('reminds', 0.47635403275489807),\n",
       " ('weird', 0.46717914938926697),\n",
       " ('reminiscent', 0.4525703489780426),\n",
       " ('funny', 0.4506691098213196),\n",
       " ('funky', 0.44231778383255005),\n",
       " ('odd', 0.4416513442993164),\n",
       " ('resemble', 0.42532044649124146)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words most similar to a given word.\n",
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "6296cb60a4d4ba8c750bd1de89017122604c38c0",
    "id": "3-IiBwrwvb1g",
    "outputId": "7f59551a-db93-4dc2-a9c0-8770fed03410"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37477237"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similaraity b/w two words\n",
    "w2v_model.wv.similarity('good','like')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f83b2a84605e54ae86f90ea86150b8891eb0271a",
    "id": "KSDerJ9Nv3j7"
   },
   "source": [
    "#### Now creating a dictionary with words in vocab and their embeddings. This will be used when we will be creating embedding matrix (for feeding to keras embedding layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "46027121588e43cc98f6e925fc3eedaa561d2802",
    "id": "xjTUnAL4zcmW",
    "outputId": "b3d4a082-1586-4657-92a9-a6decca455dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no of words : 56379\n"
     ]
    }
   ],
   "source": [
    "print(\"The no of words :\",len(vocab))\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "f1dd13a7bb306dd0002a5b51486515220d163c5e",
    "id": "RKjJClxezPSf"
   },
   "outputs": [],
   "source": [
    "# print(vocab)\n",
    "vocab=list(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "1f09f5f21f612ef5a3b59747218e6c0ccb0ad415",
    "id": "rcqASqyLwNDk",
    "outputId": "cd2f1c03-5ff9-4f05-9279-81a97c93c24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no of key-value pairs :  56379\n"
     ]
    }
   ],
   "source": [
    "word_vec_dict={}\n",
    "for word in vocab:\n",
    "  word_vec_dict[word]=w2v_model.wv.get_vector(word)\n",
    "print(\"The no of key-value pairs : \",len(word_vec_dict)) # should come equal to vocab size\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "325ae1b4b37dc7003ddc7e8a1aad7920dfbd5b75",
    "id": "Gm3rrnKZxgY9"
   },
   "outputs": [],
   "source": [
    "# # just check\n",
    "# for word in vocab[:5]:\n",
    "#   print(word_vec_dict[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3b1b7b10e439cd36ac8bb406d9cf2a3c6b41e5a"
   },
   "source": [
    "### PREPARING THE DATA FOR KERAS EMBEDDING LAYER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c11ae84127697c02d4e6f51d1af3159dacb490ae"
   },
   "source": [
    "Now we have obtained the w2v embeddings. But there are a couple of steps required by Keras embedding layer before we can move on.\n",
    "\n",
    "**Also note that since w2v embeddings have been made now ; we can preprocess our review column by using the function that we saw above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "c07b374090e27dac23d28a87fcad917d81b0975c",
    "id": "aempfGb-TNO3"
   },
   "outputs": [],
   "source": [
    "# cleaning reviews.\n",
    "df['clean_review']=df['review'].apply(clean_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a554aeb0450b5d9c1e96418ff3cd545b88814a41"
   },
   "source": [
    "#### We need to find the maximum lenght of any document or review in our case. WE will pad all reviews to have this same length.This will be required by Keras embedding layer. Must check [this](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer) kernel on Kaggle for a wonderful explanation of keras embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "d2653074d7010e129645d150ff57a849b1f69cb0",
    "id": "4DmmFNv0TlED",
    "outputId": "4181171a-936b-481a-afce-d9da33d9f831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1564\n"
     ]
    }
   ],
   "source": [
    "# number of unique words = 56379.\n",
    "\n",
    "# now since we will have to pad we need to find the maximum lenght of any document.\n",
    "\n",
    "maxi=-1\n",
    "for i,rev in enumerate(df['clean_review']):\n",
    "  tokens=rev.split()\n",
    "  if(len(tokens)>maxi):\n",
    "    maxi=len(tokens)\n",
    "print(maxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "040ec19e7f33e175cfa8a15cd31bd11e48c053db"
   },
   "source": [
    "#### Now we integer encode the words in the reviews using Keras tokenizer. \n",
    "\n",
    "**Note that there two important variables: which are the vocab_size which is the total no of unique words while the second is max_doc_len which is the length of every document after padding. Both of these are required by the Keras embedding layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "64fe854ee56f7ac7f1881e690e7d98b5fb10520f",
    "id": "9pw6qdOfntcS"
   },
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(df['clean_review'])\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "encd_rev = tok.texts_to_sequences(df['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "865e1a04cd9b78a021b1e1e788af45b244a872cf",
    "id": "S6K-WwbSnnaa"
   },
   "outputs": [],
   "source": [
    "max_rev_len=1565  # max lenght of a review\n",
    "vocab_size = len(tok.word_index) + 1  # total no of words\n",
    "embed_dim=300 # embedding dimension as choosen in word2vec constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "800f7cf8e872c92ff3930eea9c1dd564560225da",
    "id": "h9NqY0ztof_Z",
    "outputId": "4b56d53b-bcf6-4c7b-ce99-7e4f803c2c86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1565)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now padding to have a amximum length of 1565\n",
    "pad_rev= pad_sequences(encd_rev, maxlen=max_rev_len, padding='post')\n",
    "pad_rev.shape   # note that we had 100K reviews and we have padded each review to have  a lenght of 1565 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a6d9bfc59482176a7f5916a7f4ca68aa094e8a8"
   },
   "source": [
    "### CREATING THE EMBEDDING MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4fa0d53b949cb62e6111a2d07245d8f3ed0a8a4c"
   },
   "source": [
    "#### Now we need to pass the w2v word embeddings to the embedding layer in Keras. For this we will create the embedding matrix and pass it as 'embedding_initializer' parameter to the layer.\n",
    "\n",
    "**The embedding matrix will be of dimensions (vocab_size,embed_dim) where the word_index of each word from keras tokenizer is its index into the matrix and the corressponding entry is its w2v vector ;)**\n",
    "\n",
    "**Note that there may be words which will not be present in embeddings learnt by the w2v model. The embedding matrix entry corressponding to those words will be a vector of all zeros.**\n",
    "\n",
    "**Also note that if u are thinkng why won't a word be present then it is bcoz now we have learnt on out own corpus but if we use pre-trained embedding then it may happen that some words specific to our dataset aren't present then in those cases we may use a fixed vector of zeros to denote all those words that earen;t present in th pre-trained embeddings. Also note that it may also happen that some words are not present ifu have filtered some words by setting min_count in w2v constructor.\n",
    "  **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "f751ae6da742675fc4789fa33421cd87223d651d",
    "id": "GlJCOsmfxu-S"
   },
   "outputs": [],
   "source": [
    "# now creating the embedding matrix\n",
    "embed_matrix=np.zeros(shape=(vocab_size,embed_dim))\n",
    "for word,i in tok.word_index.items():\n",
    "  embed_vector=word_vec_dict.get(word)\n",
    "  if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
    "    embed_matrix[i]=embed_vector\n",
    "  # if word is not found then embed_vector corressponding to that vector will stay zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "f2053c087cc7c811b4f7cd656ffd078833236632",
    "id": "HaH6QTc865Bn",
    "outputId": "c70d670a-8ee6-430b-df04-1dd9d97cf20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.58924913e-01  5.17183661e-01  2.84342825e-01 -8.43485951e-01\n",
      " -1.76445138e+00  1.04833543e-01 -5.44512987e-01 -1.27789557e+00\n",
      " -1.05731893e+00  3.97830606e-01 -1.31565022e+00 -1.86238980e+00\n",
      " -4.09787983e-01  6.21800125e-01 -8.14280629e-01 -2.19465470e+00\n",
      "  1.02216482e+00 -1.01699245e+00  3.95713359e-01  2.14649892e+00\n",
      " -2.63330966e-01  1.49876785e+00  1.25377262e+00  9.05977249e-01\n",
      " -1.30281627e+00  7.66059309e-02 -2.05804515e+00 -6.89648911e-02\n",
      "  1.68997777e+00 -4.65638459e-01 -1.70714545e+00 -1.46614492e-01\n",
      " -1.19906855e+00 -4.93038177e-01  2.75440216e-01  5.66804588e-01\n",
      " -9.50901628e-01  1.00748718e+00 -1.15844476e+00 -1.85627329e+00\n",
      " -2.91730785e+00 -2.51066476e-01  1.41021997e-01  1.27498353e+00\n",
      " -3.23000789e+00 -1.52722085e+00  9.43417132e-01  9.49800730e-01\n",
      " -4.90911663e-01  1.15841830e+00 -1.33654615e-02  6.70593560e-01\n",
      "  1.88699424e+00  2.82784402e-01  1.43955243e+00 -1.00289214e+00\n",
      " -1.15574777e+00 -2.36216331e+00 -6.26569614e-02 -4.15267372e+00\n",
      " -1.31225026e+00  2.11252999e+00 -1.78537989e+00  1.98006749e+00\n",
      "  1.92102981e+00 -1.06543922e+00  5.59252083e-01  1.38939664e-01\n",
      "  8.35343227e-02 -8.63865972e-01  7.28792071e-01  1.85496354e+00\n",
      " -1.00144374e-04 -6.59257591e-01 -2.45052528e+00  1.18177807e+00\n",
      " -2.09254336e+00  2.61724830e+00  1.56020892e+00 -2.79974580e-01\n",
      "  1.73101735e+00 -1.17976987e+00 -7.24631131e-01 -2.74126619e-01\n",
      "  2.47088814e+00 -2.14096880e+00 -7.83823371e-01  8.21797788e-01\n",
      "  7.91047335e-01 -9.60933864e-01 -1.29810297e+00  1.92618108e+00\n",
      " -1.30332553e+00  1.26347765e-01  7.32063591e-01  9.54015970e-01\n",
      " -5.43584228e-01 -7.20193326e-01  2.54540771e-01 -6.97456479e-01\n",
      " -2.72787660e-01 -1.58049297e+00 -1.27847278e+00 -4.69068378e-01\n",
      "  6.85136318e-01  8.24712336e-01 -9.57218409e-02  8.78163099e-01\n",
      "  1.05660748e+00  1.48800671e+00  1.38244915e+00  4.95599240e-01\n",
      "  3.44691575e-01  1.17310858e+00 -1.32794058e+00 -2.40167212e+00\n",
      " -1.43358243e+00 -3.02868867e+00  2.40260792e+00  2.44727182e+00\n",
      "  1.47271931e-01  1.52375862e-01  1.32035518e+00  1.48699427e+00\n",
      "  9.53567564e-01 -1.01672518e+00  1.93439174e+00 -1.30301213e+00\n",
      " -8.32414255e-02 -1.54847801e+00  1.98542833e+00  1.82541594e-01\n",
      " -1.05382729e+00 -1.08416748e+00 -1.37494874e+00 -3.24333906e-01\n",
      " -6.75913513e-01  9.14912820e-01 -5.44668078e-01 -1.62394047e+00\n",
      " -1.92389131e+00  8.48540962e-01  5.23214698e-01  3.33213836e-01\n",
      "  1.58091232e-01 -1.58284032e+00 -1.39121389e+00  5.94865561e-01\n",
      " -7.24669158e-01 -4.27376151e-01  9.29375529e-01  1.49860978e+00\n",
      "  4.30870771e-01  2.48896551e+00  1.17717063e+00  5.13523340e-01\n",
      "  6.68074071e-01  5.25011495e-02 -1.14285564e+00 -1.01912761e+00\n",
      " -1.05788052e+00  6.62412405e-01 -1.80851305e+00 -9.42955315e-01\n",
      " -9.29161429e-01  1.32559729e+00 -7.55987346e-01  1.73965236e-03\n",
      "  9.08559024e-01  3.09343547e-01 -7.22450733e-01  9.68200207e-01\n",
      "  2.73051113e-01 -1.65743366e-01  1.81692612e+00 -2.03810716e+00\n",
      " -1.90544856e+00 -3.97856116e-01  7.15248466e-01  1.09668624e+00\n",
      " -7.22211003e-01  1.64025354e+00 -2.49036217e+00  2.25683808e+00\n",
      " -4.69402909e-01 -2.94266772e+00  6.94414675e-01  1.60169733e+00\n",
      " -9.24499989e-01 -1.80268312e+00  1.29871118e+00  2.15761089e+00\n",
      "  1.34871769e+00 -9.77408588e-01  7.54243255e-01  1.36920619e+00\n",
      "  7.05927610e-01 -2.01895213e+00  3.09355587e-01 -1.00905216e+00\n",
      "  4.94832695e-02  1.75867748e+00  1.30671144e+00 -2.36080670e+00\n",
      " -3.18231404e-01  2.98393416e+00 -7.67718911e-01 -1.97328389e+00\n",
      " -2.18812063e-01 -1.62744212e+00 -1.86581150e-01 -6.81089088e-02\n",
      "  5.73100686e-01  5.92487276e-01 -9.95296955e-01 -3.66460323e-01\n",
      "  1.63105106e+00  5.67411900e-01 -1.61236572e+00  1.76400828e+00\n",
      "  4.07378167e-01  2.97766656e-01 -3.86992598e+00 -3.73642057e-01\n",
      " -3.07313472e-01  7.04577744e-01  1.56204927e+00 -3.38107139e-01\n",
      " -1.21843302e+00 -9.46027160e-01 -9.16490793e-01  2.47544289e+00\n",
      " -7.21907318e-02 -1.36405015e+00  1.75509167e+00  1.63244700e+00\n",
      "  1.91186416e+00 -2.34485388e+00 -6.63230419e-01  2.47511402e-01\n",
      " -1.86960101e+00 -9.99461561e-02  1.78865802e+00 -1.31455526e-01\n",
      "  4.60283667e-01  1.31438243e+00  1.38640785e+00 -5.30584693e-01\n",
      " -5.62825918e-01 -5.24846852e-01  1.17610359e+00  5.80391586e-01\n",
      " -1.64415205e+00  1.51226890e+00  6.60670519e-01 -2.18284106e+00\n",
      "  2.97734499e-01 -3.74227196e-01 -1.25074708e+00  1.81669247e+00\n",
      "  1.31356925e-01  1.23401392e+00  4.91397828e-01  2.28657164e-02\n",
      " -5.80495298e-01 -2.40258217e+00 -8.53681684e-01 -1.02160744e-01\n",
      " -8.44187915e-01  7.19104856e-02  1.56572640e+00  1.04723859e+00\n",
      "  7.61298060e-01 -3.64791393e-01  7.38081217e-01 -1.18163812e+00\n",
      " -3.91969770e-01 -8.60806882e-01  9.31162953e-01 -1.11602746e-01\n",
      "  4.41051483e-01  8.98824453e-01 -2.17192101e+00  1.65863574e+00\n",
      " -3.70126933e-01  1.07589260e-01 -2.52909064e+00  1.87170815e+00\n",
      "  1.87177926e-01  1.86241782e+00  6.42070830e-01  4.01060343e-01\n",
      " -1.91582784e-01  2.37141065e-02 -1.23775351e+00 -1.12118268e+00\n",
      "  2.40052730e-01 -2.82249212e+00  1.72710729e+00 -1.41050613e+00]\n"
     ]
    }
   ],
   "source": [
    "# checking.\n",
    "print(embed_matrix[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "922f83048b341c34a28b49e6114e612b507b87e6",
    "id": "rwoFFrW59Rjk"
   },
   "source": [
    "### PREPARING TRAIN AND VALIDATION SETS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "ff5829cf69c1ac4fdb14c24d9be3c14683363a45",
    "id": "jNGi31a5AHxC"
   },
   "outputs": [],
   "source": [
    "# prepare train and val sets first\n",
    "Y=keras.utils.to_categorical(df['sentiment'])  # one hot target as required by NN.\n",
    "x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1e22e81bc6a49adaad7c4ca4d36e058f6d672f3"
   },
   "source": [
    "### BUILDING A MODEL AND FINALLY PERFORMING TEXT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bfb5c6b91f5bcae59c62b5b6650a8040769cf8b8"
   },
   "source": [
    "Having done all the pre-requisites we finally move onto make model in Keras .\n",
    "\n",
    "**Note that I have commented the LSTM layer as including it causes the trainig loss to be stucked at a value of about 0.6932. I don;t know why ;(.**\n",
    "\n",
    "**In case someone knows please comment below. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "3b1b2e253f0a76b487e2732ffc5f1b32c5e8e0c0",
    "id": "ZmsnBmEf-Ktr"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=max_rev_len,embeddings_initializer=Constant(embed_matrix)))\n",
    "# model.add(CuDNNLSTM(64,return_sequences=False)) # loss stucks at about \n",
    "model.add(Flatten())\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "# model.add(Dense(16,activation='relu'))\n",
    "# model.add(Dropout(0.20))\n",
    "model.add(Dense(2,activation='sigmoid'))  # sigmod for bin. classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "915d060b56a109792dfff8a8283e6b94dc483773"
   },
   "source": [
    "#### Let us now print a summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "0bdbc7d142d477bc129b89ab8dc7a606ab07c8b5",
    "id": "arGSpbbd_0E1",
    "outputId": "ee336fa8-6bc6-4b50-84a6-ccefc544ead9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1565, 300)         16914000  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 469500)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                7512016   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 24,426,050\n",
      "Trainable params: 24,426,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "087c686671c54c334bd605a1b76738156f9b975d",
    "id": "rtVq2ncD_2a1"
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "ed1c7cc4bb2219ca59cf3c777c0222bc2a059083",
    "id": "xn6g-HknAe9A"
   },
   "outputs": [],
   "source": [
    "# specify batch size and epocj=hs for training.\n",
    "epochs=5\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "89e963f56a787c2096706fee9357afcf6121290a",
    "id": "gvDNMDeWADh9",
    "outputId": "443f2050-c627-4630-b8f4-3324f7e09c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 173s 136ms/step - loss: 0.5057 - accuracy: 0.7568 - val_loss: 0.4144 - val_accuracy: 0.8207\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 162s 130ms/step - loss: 0.4058 - accuracy: 0.8184 - val_loss: 0.3891 - val_accuracy: 0.8346\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 163s 130ms/step - loss: 0.3567 - accuracy: 0.8406 - val_loss: 0.3882 - val_accuracy: 0.8406\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 169s 136ms/step - loss: 0.3159 - accuracy: 0.8554 - val_loss: 0.4103 - val_accuracy: 0.8427\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 165s 132ms/step - loss: 0.2886 - accuracy: 0.8661 - val_loss: 0.4290 - val_accuracy: 0.8382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a9f262b8c8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model.\n",
    "model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('weights/word2vec-lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37b2ea17080f0d7f04d8da9a09db76333215eb11",
    "id": "eOsqzdCsD-ia"
   },
   "source": [
    "#### Note that loss as well as val_loss is  is still deceasing. You can train for more no of epochs but I am not so patient ;)\n",
    "\n",
    "**The final accuracy after 5 epochs is about 84% which is pretty decent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1c2fdb1c54cae3dd9303ed07434da397e0d6d12"
   },
   "source": [
    "### FURTHER IDEAS : -\n",
    "\n",
    "1) ProductId and UserId can be used to track the general ratings of a given product and also to track the review patter of a particular user as if he is strict in reviwing or not.\n",
    " \n",
    "\n",
    "2) Helpfulness feature may tell about the product. This is because gretare the no of people talking about reviews, the mre stronger or critical it is expected to be.\n",
    "\n",
    "3) Summary column can also give a hint.\n",
    "\n",
    "4) One can also try the pre-trained embeddings like Glove word vectors etc...\n",
    "\n",
    "5) Lastly tuning the n/w hyperparameters is always an option;).\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6813c2ddd2a5746eeecbdb418bcff4174202cb90a311d03b6a1125d4f8afb7f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
